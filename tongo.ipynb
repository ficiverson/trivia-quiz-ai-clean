{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . | grep -E \"(__pycache__|\\.pyc|\\.pyo$)\" | xargs rm -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --force-reinstall neo4j langchain-openai langchain langchain-community langchain-huggingface pandas tabulate pydub ffprobe-python feedparser elevenlabs tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup env vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1724076792059
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Validate environment variables\n",
    "required_env_vars = [\n",
    "            \"AZURE_OPENAI_API_KEY\",\n",
    "            \"AZURE_OPENAI_ENDPOINT\",\n",
    "            \"AZURE_OPENAI_API_VERSION\",\n",
    "            \"AZURE_OPENAI_DEPLOYMENT_NAME\"\n",
    "        ]\n",
    "        \n",
    "# Print environment variables for debugging\n",
    "print(\"Azure OpenAI Configuration:\")\n",
    "for var in required_env_vars:\n",
    "    print(f\"{var}: {os.getenv(var)}\")\n",
    "        \n",
    "missing_vars = [var for var in required_env_vars if not os.getenv(var)]\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required environment variables: {', '.join(missing_vars)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the LLM and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "import json\n",
    "\n",
    "llm_model = AzureChatOpenAI(\n",
    "                deployment_name=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT_NAME\", \"default_value\"),\n",
    "                openai_api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\", \"default_value\"),\n",
    "                azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"default_value\"),\n",
    "                api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"default_value\")\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the Reflection agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    loop_needed: bool\n",
    "    content: str\n",
    "    updated_content : str\n",
    "\n",
    "\n",
    "class Reflect(BaseModel):\n",
    "    \"\"\"Verify if the correct answer is correct given a json with the question, correct answer and incorrect answers. \n",
    "    Keep always Spanish as the input and output language. \n",
    "    If it is not, return True. If it is correct, return False in the field loop_needed.\n",
    "    The output field should contain a new version of the JSON with the correct answer fixed. \n",
    "    JSON Schema:\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The trivia question\"\n",
    "            },\n",
    "            \"correctAnswer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The correct answer\"\n",
    "            },\n",
    "            \"incorrectAnswers\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"description\": \"List of incorrect answers\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"question\",\n",
    "            \"correctAnswer\",\n",
    "            \"incorrectAnswers\"\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    loop_needed : bool = Field(description=\"True if the correct answer is not correct\")\n",
    "    output : str = Field(description=\"The json with the format\")\n",
    "    reason : str = Field(description=\"Explain why the answer is correct or not\")\n",
    "\n",
    "is_a_loop_needed = llm_model.with_structured_output(\n",
    "            Reflect\n",
    "        )\n",
    "\n",
    "def reflect(state: State):\n",
    "        if(state.get(\"loop_needed\", False) or False):\n",
    "            return {\n",
    "                \"loop_needed\": False,\n",
    "                \"updated_content\" : state.get(\"updated_content\",\"\")\n",
    "            }  \n",
    "        else:\n",
    "            result = is_a_loop_needed.invoke(state[\"content\"])\n",
    "            print(result.loop_needed)\n",
    "            print(result.reason)\n",
    "            print(result.output)\n",
    "            return {\n",
    "                \"loop_needed\": result.loop_needed,\n",
    "                \"updated_content\" : result.output\n",
    "            }\n",
    "\n",
    "def reflect_conditional_context(state: State) -> Literal[\"generate_question\", END]:\n",
    "        if state.get(\"loop_needed\", False):\n",
    "            return \"generate_question\"\n",
    "        else:\n",
    "            return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the generation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1724076718593
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#model to format the reponse\n",
    "\n",
    "class GenerateQuestion(BaseModel):\n",
    "    \"\"\"You are a trivia expert generating questions for a trivia game. Generate a structured response based on the given category. \n",
    "    The category is coming in Spanish. \n",
    "    The output field should contain a new version of the JSON with the correct answer fixed. \n",
    "    Keep Spanish as the output language.\n",
    "    When the category is \"cine y tv\", the question should be about a movie or a tv show.\n",
    "    When the category is \"geografía\", the question should be about a country, a city or a landmark.\n",
    "    When the category is \"historia\", the question should be about a historical event, a person or a place.\n",
    "    When the category is \"deportes\", the question should be about a sport, a player or a team.\n",
    "    When the category is \"corazón\", the question should be about a romantic relationship, a break-up or a love story of famous people.\n",
    "    When the category is \"videojuegos\", the question should be about a video game, a character or a game console.\n",
    "    When the category is \"tongurso\", one of the incorrect answers should be a typo of the correct answer and you can pick a random category.\n",
    "    JSON Schema:\n",
    "    {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The trivia question\"\n",
    "            },\n",
    "            \"correctAnswer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The correct answer\"\n",
    "            },\n",
    "            \"incorrectAnswers\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"description\": \"List of incorrect answers\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"question\",\n",
    "            \"correctAnswer\",\n",
    "            \"incorrectAnswers\"\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    output : str = Field(description=\"The json of the response withut special characters\")\n",
    "\n",
    "def generate_question(state: State):\n",
    "    extracted_question = llm_model.with_structured_output(\n",
    "                GenerateQuestion\n",
    "            )\n",
    "    if(len(state.get(\"updated_content\",\"\"))>0 or False):\n",
    "        return {\"content\":state.get(\"updated_content\",\"\")}\n",
    "    else: \n",
    "        result = extracted_question.invoke(state[\"messages\"])\n",
    "        print(result.output)\n",
    "        return {\"content\":result.output}\n",
    "        #return {\"content\":\"{\\\"question\\\":\\\"¿Cuál es el río más largo del mundo?\\\",\\\"correctAnswer\\\":\\\"Río Amazonas\\\",\\\"incorrectAnswers\\\":[\\\"Río Nilo\\\",\\\"Río Mississippi\\\",\\\"Río Ebro\\\"]}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"generate_question\", generate_question)\n",
    "graph_builder.add_node(\"reflect\", reflect)\n",
    "\n",
    "graph_builder.add_edge(START, \"generate_question\")\n",
    "graph_builder.add_edge(\"generate_question\", \"reflect\")\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"reflect\", reflect_conditional_context\n",
    "    )\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    print \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exceute the multi agent system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke({\"messages\":[\"cine y tv\"]})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = json.loads(result.get(\"content\", \"{}\"))  # Convert JSON string to dictionary\n",
    "print(content_dict)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
